{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Asaf Shtrul & Kim Boren - DL Assignment 1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE6qcw_j8Pi2"
      },
      "source": [
        "In this homework assignment, you are requested to implement a full backprop algorithm using only *numpy*.\n",
        "\n",
        "- We assume sigmoid activation across all layers.\n",
        "- We assume a single value in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV4RvXYL8k85"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "np.random.seed(42)\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRml6glFIPCa"
      },
      "source": [
        "The following class represents a simple feed forward network with multiple layers. The network class provides methods for running forward and backward for a single instance, throught the network. You should implement the methods (indicated with TODO), that performs forward and backward for an entire batch. Note, the idea is to use matrix multiplications, and not running standard loops over the instances in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLdNoCt58qg5"
      },
      "source": [
        "\n",
        "class MyNN:\n",
        "  def __init__(self, learning_rate, layer_sizes):\n",
        "    '''\n",
        "    learning_rate - the learning to use in backward\n",
        "    layer_sizes - a list of numbers, each number repreents the nuber of neurons\n",
        "                  to have in every layer. Therfore, the length of the list \n",
        "                  represents the number layers this network has.\n",
        "    '''\n",
        "    self.learning_rate = learning_rate\n",
        "    self.layer_sizes = layer_sizes\n",
        "    self.model_params = {}\n",
        "    self.memory = {}\n",
        "    self.grads = {}\n",
        "    \n",
        "    # Initializing weights\n",
        "    for layer_index in range(len(layer_sizes) - 1):\n",
        "      W_input = layer_sizes[layer_index + 1]\n",
        "      W_output = layer_sizes[layer_index]\n",
        "      self.model_params['W_' + str(layer_index + 1)] = np.random.randn(W_input, W_output) * 0.1\n",
        "      self.model_params['b_' + str(layer_index + 1)] = np.random.randn(W_input) * 0.1\n",
        "      \n",
        "      \n",
        "  def forward_single_instance(self, x):    \n",
        "    a_i_1 = x\n",
        "    self.memory['a_0'] = x\n",
        "    for layer_index in range(len(self.layer_sizes) - 1):\n",
        "      W_i = self.model_params['W_' + str(layer_index + 1)]\n",
        "      b_i = self.model_params['b_' + str(layer_index + 1)]\n",
        "      z_i = np.dot(W_i, a_i_1) + b_i\n",
        "      a_i = 1/(1+np.exp(-z_i))\n",
        "      self.memory['a_' + str(layer_index + 1)] = a_i\n",
        "      a_i_1 = a_i\n",
        "    return a_i_1\n",
        "  \n",
        "  \n",
        "  def log_loss(self, y_hat, y):\n",
        "    '''\n",
        "    Logistic loss, assuming a single value in y_hat and y.\n",
        "    '''\n",
        "    m = y_hat[0]\n",
        "    cost = -y[0]*np.log(y_hat[0]) - (1 - y[0])*np.log(1 - y_hat[0])\n",
        "    return cost\n",
        "  \n",
        "  \n",
        "  def backward_single_instance(self, y):\n",
        "    a_output = self.memory['a_' + str(len(self.layer_sizes) - 1)]\n",
        "    dz = a_output - y\n",
        "     \n",
        "    for layer_index in range(len(self.layer_sizes) - 1, 0, -1):\n",
        "      print(layer_index)\n",
        "      a_l_1 = self.memory['a_' + str(layer_index - 1)]\n",
        "      dW = np.dot(dz.reshape(-1, 1), a_l_1.reshape(1, -1))\n",
        "      self.grads['dW_' + str(layer_index)] = dW\n",
        "      W_l = self.model_params['W_' + str(layer_index)]\n",
        "      dz = (a_l_1 * (1 - a_l_1)).reshape(-1, 1) * np.dot(W_l.T, dz.reshape(-1, 1))\n",
        "      # TODO-1: calculate and memorize db as well.\n",
        "      db = dz #done-1 \n",
        "  \n",
        "  # TODO-2: update weights with grads\n",
        "  def update(self): #done-2\n",
        "     for layer_index in range(len(self.layer_sizes)-1):\n",
        "      self.model_params['W_' + str(layer_index + 1)] -= (self.learning_rate * self.grads['dW_' + str(layer_index + 1)])\n",
        "      self.model_params['b_' + str(layer_index + 1)] -= (self.learning_rate * self.grads['db_' + str(layer_index + 1)])\n",
        " \n",
        "  \n",
        "  # TODO-3: implement forward for a batch X.shape = (network_input_size, number_of_instance)\n",
        "  def forward_batch(self, X): #done-3\n",
        "    A_i_1 = X\n",
        "    self.memory['A_0'] = X\n",
        "    for layer_index in range(len(self.layer_sizes) - 1):\n",
        "      W_i = self.model_params['W_' + str(layer_index + 1)]\n",
        "      b_i = self.model_params['b_' + str(layer_index + 1)]\n",
        "      Z_i = np.dot(W_i, A_i_1) + b_i.reshape(-1,1)\n",
        "      A_i = 1/(1+np.exp(-Z_i))\n",
        "      self.memory['A_' + str(layer_index + 1)] = A_i\n",
        "      A_i_1 = A_i\n",
        "    return A_i_1\n",
        "  \n",
        "  # TODO-4: implement backward for a batch y.shape = (1, number_of_instance)\n",
        "  def backward_batch(self, y): #done-4\n",
        "    A_output = self.memory['A_' + str(len(self.layer_sizes) - 1)]\n",
        "    dZ = A_output - y\n",
        "    m = (y.shape[1])\n",
        " \n",
        "    for layer_index in range(len(self.layer_sizes) - 1, 0, -1):\n",
        "      A_l_1 = self.memory['A_' + str(layer_index-1)]\n",
        "      dW = (1/m)*(np.dot(dZ, A_l_1.T))\n",
        "      self.grads['dW_' + str(layer_index)] = dW\n",
        "      W_l = self.model_params['W_' + str(layer_index)]\n",
        "      db = dZ.T.mean(0)\n",
        "      self.grads['db_' + str(layer_index)] = db\n",
        "      dZ = (A_l_1 * (1 - A_l_1))* np.dot(W_l.T, dZ)\n",
        "  \n",
        "  # TODO-5: implement log_loss_batch, for a batch of instances\n",
        "  def log_loss_batch(self, y_hat, y): #done-5\n",
        "    m = y.shape[1]\n",
        "    cost =  (np.dot(y, np.log(y_hat).T) + np.dot(1 - y, np.log(1 - y_hat).T))\n",
        "    return cost.sum() * (-1/m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qib6W4QXO644"
      },
      "source": [
        "nn = MyNN(0.01, [3, 2, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nQR8QllPf_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d630d6f-5a86-4716-987e-cccaf848f514"
      },
      "source": [
        "nn.model_params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W_1': array([[ 0.04967142, -0.01382643,  0.06476885],\n",
              "        [ 0.15230299, -0.02341534, -0.0234137 ]]),\n",
              " 'W_2': array([[-0.04694744,  0.054256  ]]),\n",
              " 'b_1': array([0.15792128, 0.07674347]),\n",
              " 'b_2': array([-0.04634177])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXiyn-yrPC6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439ebe26-9c47-47aa-8820-6d482b72c50d"
      },
      "source": [
        "x = np.random.randn(3)\n",
        "y = np.random.randn(1)\n",
        "\n",
        "y_hat = nn.forward_single_instance(x)\n",
        "print(y_hat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.48946]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5M50i3plclj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4effaa76-ec54-46ce-8c4e-3d8b98abcd36"
      },
      "source": [
        "nn.backward_single_instance(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWnZB1YmYnIt"
      },
      "source": [
        "def train(X, y, epochs, batch_size):\n",
        "  '''\n",
        "  Train procedure, please note the TODOs inside\n",
        "  '''\n",
        "  loss_per_epoch = []\n",
        "  for e in range(1, epochs + 1):\n",
        "    epoch_loss = 0\n",
        "    X_s, y_s = shuffle(X.T, y.T)\n",
        "    batches_X = np.array_split(X_s ,batch_size)  # TODO: - DONE\n",
        "    batches_y = np.array_split(y_s, batch_size)\n",
        "\n",
        "\n",
        "    for X_b, y_b in zip(batches_X, batches_y):\n",
        "      y_hat = nn.forward_batch(X_b.T)\n",
        "      epoch_loss += nn.log_loss_batch(y_hat, y_b.T)\n",
        "      nn.backward_batch(y_b.T)\n",
        "      nn.update()\n",
        "    loss_per_epoch.append(epoch_loss / len(batches_X))\n",
        "    print(f'Epoch {e}, loss={epoch_loss/len(batches_X)}')\n",
        "  return loss_per_epoch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE1ydWlatkty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b32e87-daf7-492c-c7c7-e3c9cce4abae"
      },
      "source": [
        "# TODO-8: Make sure the following network trains properly\n",
        "#done-8, work good\n",
        "nn = MyNN(0.001, [6, 4, 3, 1])\n",
        "\n",
        "X = np.random.randn(6, 100)\n",
        "y = np.random.randn(1, 100)\n",
        "batch_size = 8\n",
        "epochs = 2\n",
        "\n",
        "train(X, y, epochs, batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, loss=0.7034174436116096\n",
            "Epoch 2, loss=0.6983151243233774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7034174436116096, 0.6983151243233774]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dY4scUksulC"
      },
      "source": [
        "#TODO: train on an external dataset\n",
        "\n",
        "Train on the Bike Sharing dataset, using the same split as in *DL Notebook 4 - logistic regression*.\n",
        "Use the following features from the data:\n",
        "\n",
        "* temp\n",
        "* atemp\n",
        "* hum\n",
        "* windspeed\n",
        "* weekday\n",
        "\n",
        "The response variable is, like in Notebook 4, raw[\"success\"] = raw[\"cnt\"] > (raw[\"cnt\"].describe()[\"mean\"]).\n",
        "\n",
        "The architecture of the network should be: [5, 40, 30, 10, 7, 5, 3, 1].\n",
        "\n",
        "Use batch_size=8, and train it for 100 epochs on the train set (based on the split as requested above).\n",
        "\n",
        "Then, plot loss per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80rAHH3q_-ok",
        "outputId": "a9ab8a8f-4f7a-4e08-853b-e7dde8436c25"
      },
      "source": [
        "!git clone https://github.com/kfirbar/course-ml-data.git\n",
        "raw = pd.read_csv('course-ml-data/Bike-Sharing-Dataset 2/day.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'course-ml-data'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 24 (delta 3), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "Ir3vXMIsC2do",
        "outputId": "cb381b05-9322-4355-d3d0-d67423930489"
      },
      "source": [
        "raw.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "      <th>cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.344167</td>\n",
              "      <td>0.363625</td>\n",
              "      <td>0.805833</td>\n",
              "      <td>0.160446</td>\n",
              "      <td>331</td>\n",
              "      <td>654</td>\n",
              "      <td>985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2011-01-02</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.363478</td>\n",
              "      <td>0.353739</td>\n",
              "      <td>0.696087</td>\n",
              "      <td>0.248539</td>\n",
              "      <td>131</td>\n",
              "      <td>670</td>\n",
              "      <td>801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2011-01-03</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.196364</td>\n",
              "      <td>0.189405</td>\n",
              "      <td>0.437273</td>\n",
              "      <td>0.248309</td>\n",
              "      <td>120</td>\n",
              "      <td>1229</td>\n",
              "      <td>1349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2011-01-04</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.212122</td>\n",
              "      <td>0.590435</td>\n",
              "      <td>0.160296</td>\n",
              "      <td>108</td>\n",
              "      <td>1454</td>\n",
              "      <td>1562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2011-01-05</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.226957</td>\n",
              "      <td>0.229270</td>\n",
              "      <td>0.436957</td>\n",
              "      <td>0.186900</td>\n",
              "      <td>82</td>\n",
              "      <td>1518</td>\n",
              "      <td>1600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   instant      dteday  season  yr  ...  windspeed  casual  registered   cnt\n",
              "0        1  2011-01-01       1   0  ...   0.160446     331         654   985\n",
              "1        2  2011-01-02       1   0  ...   0.248539     131         670   801\n",
              "2        3  2011-01-03       1   0  ...   0.248309     120        1229  1349\n",
              "3        4  2011-01-04       1   0  ...   0.160296     108        1454  1562\n",
              "4        5  2011-01-05       1   0  ...   0.186900      82        1518  1600\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpvCM60HC98J",
        "outputId": "763dc4b8-ba44-421e-bfaa-f26419994db4"
      },
      "source": [
        "raw[\"cnt\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     731.000000\n",
              "mean     4504.348837\n",
              "std      1937.211452\n",
              "min        22.000000\n",
              "25%      3152.000000\n",
              "50%      4548.000000\n",
              "75%      5956.000000\n",
              "max      8714.000000\n",
              "Name: cnt, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4GZHQh_EQOk"
      },
      "source": [
        "raw[\"success\"] = raw[\"cnt\"] > (raw[\"cnt\"].describe()[\"mean\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLko6JeDERR4"
      },
      "source": [
        "batch_size = 8\n",
        "epochs = 100\n",
        "\n",
        "nn = MyNN(0.003, [5, 40, 30, 10, 7, 5, 3, 1]) #After playing a bit I saw that Learning Rate 0.003 makes the curve smoother than 0.001/0.01 :)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MviSXc9EZYN",
        "outputId": "b9b623fa-fab8-4dc7-b265-a02b1b66f7a4"
      },
      "source": [
        "y = raw[\"success\"]\n",
        "\n",
        "X = raw[[\"temp\", \"atemp\", \"hum\", \"windspeed\", \"weekday\"]]\n",
        "  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "lr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0YeuBBuuEsad",
        "outputId": "89426469-ea4b-4add-a47d-3181c4322311"
      },
      "source": [
        "X_train = np.array(X_train.T)\n",
        "X_test = np.array(X_test.T)\n",
        "y_train = np.array(y_train).reshape(1, -1)\n",
        "y_test = np.array(y_test).reshape(1, -1)\n",
        "loss_per_epoch = train(X_train, y_train, epochs, batch_size)\n",
        "plt.plot(loss_per_epoch)\n",
        "plt.xlabel('Epoch number')\n",
        "plt.ylabel(\"Loss  magnitude\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, loss=0.6959325023014535\n",
            "Epoch 2, loss=0.6957999917796989\n",
            "Epoch 3, loss=0.6956995414411546\n",
            "Epoch 4, loss=0.6955793573004388\n",
            "Epoch 5, loss=0.6954683285124046\n",
            "Epoch 6, loss=0.6953652870889095\n",
            "Epoch 7, loss=0.6952591770946386\n",
            "Epoch 8, loss=0.6951652043652375\n",
            "Epoch 9, loss=0.6950429492662691\n",
            "Epoch 10, loss=0.6949389188769175\n",
            "Epoch 11, loss=0.6948499998333958\n",
            "Epoch 12, loss=0.6947445011051451\n",
            "Epoch 13, loss=0.6946487295311816\n",
            "Epoch 14, loss=0.69457204486831\n",
            "Epoch 15, loss=0.6944656412169421\n",
            "Epoch 16, loss=0.694379897279249\n",
            "Epoch 17, loss=0.6942852599042794\n",
            "Epoch 18, loss=0.6942082930698552\n",
            "Epoch 19, loss=0.6941154923230826\n",
            "Epoch 20, loss=0.6940483954841619\n",
            "Epoch 21, loss=0.6939699962273347\n",
            "Epoch 22, loss=0.6938772048600933\n",
            "Epoch 23, loss=0.6937970794208473\n",
            "Epoch 24, loss=0.6937214770306751\n",
            "Epoch 25, loss=0.693645487077705\n",
            "Epoch 26, loss=0.6935726534323334\n",
            "Epoch 27, loss=0.6935049368328203\n",
            "Epoch 28, loss=0.6934329864470381\n",
            "Epoch 29, loss=0.6933665668413825\n",
            "Epoch 30, loss=0.693299078628149\n",
            "Epoch 31, loss=0.6932344073411114\n",
            "Epoch 32, loss=0.6931709968803751\n",
            "Epoch 33, loss=0.6930998295869677\n",
            "Epoch 34, loss=0.6930423766167012\n",
            "Epoch 35, loss=0.6929792258783254\n",
            "Epoch 36, loss=0.6929220133456347\n",
            "Epoch 37, loss=0.6928571462597735\n",
            "Epoch 38, loss=0.6928081033086092\n",
            "Epoch 39, loss=0.6927541562151488\n",
            "Epoch 40, loss=0.6926916313834983\n",
            "Epoch 41, loss=0.6926390230790191\n",
            "Epoch 42, loss=0.6925797350345322\n",
            "Epoch 43, loss=0.6925251862273913\n",
            "Epoch 44, loss=0.6924782914578399\n",
            "Epoch 45, loss=0.6924278214269871\n",
            "Epoch 46, loss=0.6923804776553562\n",
            "Epoch 47, loss=0.6923386981034316\n",
            "Epoch 48, loss=0.6922847682143478\n",
            "Epoch 49, loss=0.6922400944660971\n",
            "Epoch 50, loss=0.6921941679641199\n",
            "Epoch 51, loss=0.6921449435890188\n",
            "Epoch 52, loss=0.6920973810543215\n",
            "Epoch 53, loss=0.6920567401945465\n",
            "Epoch 54, loss=0.6920138212791184\n",
            "Epoch 55, loss=0.6919749667156391\n",
            "Epoch 56, loss=0.6919323257020512\n",
            "Epoch 57, loss=0.6919102559219202\n",
            "Epoch 58, loss=0.6918595946505232\n",
            "Epoch 59, loss=0.691823771052915\n",
            "Epoch 60, loss=0.6917849602929846\n",
            "Epoch 61, loss=0.691742605096068\n",
            "Epoch 62, loss=0.6917022683377828\n",
            "Epoch 63, loss=0.6916729706711761\n",
            "Epoch 64, loss=0.6916594920923447\n",
            "Epoch 65, loss=0.6916114199668225\n",
            "Epoch 66, loss=0.6915751563228677\n",
            "Epoch 67, loss=0.6915487138042469\n",
            "Epoch 68, loss=0.6915153590842454\n",
            "Epoch 69, loss=0.6914787152371931\n",
            "Epoch 70, loss=0.691437644881354\n",
            "Epoch 71, loss=0.6914161302236704\n",
            "Epoch 72, loss=0.6913883030605901\n",
            "Epoch 73, loss=0.6913618938480218\n",
            "Epoch 74, loss=0.691335996490612\n",
            "Epoch 75, loss=0.6912972131282104\n",
            "Epoch 76, loss=0.6912708204201552\n",
            "Epoch 77, loss=0.6912717533024433\n",
            "Epoch 78, loss=0.6912437083925325\n",
            "Epoch 79, loss=0.6912128430762532\n",
            "Epoch 80, loss=0.6911620098041532\n",
            "Epoch 81, loss=0.6911463561261074\n",
            "Epoch 82, loss=0.6911171329111137\n",
            "Epoch 83, loss=0.6910972508262437\n",
            "Epoch 84, loss=0.6910741338934617\n",
            "Epoch 85, loss=0.6910676522613454\n",
            "Epoch 86, loss=0.6910368114972189\n",
            "Epoch 87, loss=0.6910144233671839\n",
            "Epoch 88, loss=0.6909872741235837\n",
            "Epoch 89, loss=0.6909704678430715\n",
            "Epoch 90, loss=0.690954818721577\n",
            "Epoch 91, loss=0.6909324088546376\n",
            "Epoch 92, loss=0.6909226462776212\n",
            "Epoch 93, loss=0.6908925225696227\n",
            "Epoch 94, loss=0.6908748109584097\n",
            "Epoch 95, loss=0.6908602914680005\n",
            "Epoch 96, loss=0.6908383665801217\n",
            "Epoch 97, loss=0.6908141086751137\n",
            "Epoch 98, loss=0.6908022077448199\n",
            "Epoch 99, loss=0.6907771462122044\n",
            "Epoch 100, loss=0.6907579568574681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5NdGDYYEx1_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}